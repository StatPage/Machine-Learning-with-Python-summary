{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 핵심은 유닛(노드, 뉴런)이다. 유닛은 하나 이상의 입력을 받아 가중치(파라미터)를 곱한다. 가중치가 곱해진 입력에 어떤 절편을 더하고 활성화 함수에 값을 전달한다. 출력은 신경망에서 더 깊은 층에 잇는 다른 뉴런을 위해 앞으로 전달.\n",
    "- 신경망: 일련의 연결된 층으로 표현할 수 있으며 한쪽 끝에는 샘플의 특성값과 다른 한쪽에는 타깃값(예를 들면 샘플의 클래스)를 연결한 네트워크.\n",
    "- 피드포워드: 샘플의 특성값이 네트워크 앞쪽으로 주입된다는 사실을 의미. 각 층은 연속적으로 특성값을 변환하여 타깃값과 같은 최종 출력을 내는 것이 목적.\n",
    "\n",
    "피드 포워드 신경망은 세 가지 종류의 층으로 구성. 시작 부분은 입력층으로 각 유닛은 샘플의 개별 특성값. 끝은 출력층으로 은닉층의 출력의 문제를 해결할 수 있도록 변환. 입력층과 출력층 사이는 은닉층으로 입력층에서 받은 특성값을 출력층에서 처리될 때 타깃 클래스와 닮도록 연속적으로 변환. __은닉층을 많이 가진 신경망을 심층 신경망이라고 부르며 이런 애플리케이션을 딥러닝이라고 한다.__ 신경망을 만들 때 일반적으로 가우시안 분포나 균등 분포를 따르는 작은 난숫값으로 모델 파라미터 초기화.\n",
    "- 정방향 계산: 샘플 하나가 신경망에 주입되면 손실 함수를 사용해 출력 값과 샘플의 진짜 타깃값을 비교하는 것.\n",
    "- 역전파: 알고리즘 예측과 정답 간의 오차에 각 모델 파라미터가 얼마나 기여했는지 파악하기 위해 네트워크의 역방향으로 진행.\n",
    "\n",
    "신경망은 훈련 데이터에 있는 모든 샘플에 대해 정방향 계산과 역전파를 여러 번 반복하면서 학습.(에폭: 모든 샘플이 네트워크를 통과하는 것)\n",
    "모델 파라미터의 값도 반복적으로 업데이트된다. \n",
    "- 케라스: 텐서플로나 씨아노를 엔진으로 사용하는 라이브러리. 텐서 연산 같은 상세 사항은 다른 라이브러리에 위임하고 네트워크 디자인과 훈련에 집중."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망을 위한 데이터 전처리\n",
    "신경망에 사용할 데이터를 전처리 하고 싶다. → 사이킷런의 StandardScaler\n",
    "- 일반적으로 신경망의 모델 파라미터는 작은 난수로 초기화.\n",
    "- 특성값이 모델 파라미터보다 크면 종종 신경망의 성능이 나빠진다.\n",
    "- 샘플의 특성값이 개별 유닛을 통과하면서 합쳐지기 때문에 모든 특성이 같은 스케일을 가지는 것이 중요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# feature 생성\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                    [-200.2, -234.1],\n",
    "                    [5000.5, 150.1],\n",
    "                    [6000.6, -125.1],\n",
    "                    [9000.9, -673.1]])\n",
    "# 객체 생성\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# 변환\n",
    "features_stand = scaler.fit_transform(features)\n",
    "\n",
    "# 확인\n",
    "features_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성하기\n",
    "신경망 만들고 싶다. → 케라스의 Sequential 모델 사용.  \n",
    "은닉층의 유닛에 대해 기억해야 할 것.\n",
    "1. 여러 개의 값을 입력받는다.\n",
    "2. 각 입력을 모델 파라미터와 곱한다.\n",
    "3. 모든 입력에 가중치를 곱하고 절편(일반적으로 0)을 더한다.\n",
    "4. 대부분 어떤 함수(활성화 함수)를 적용한다.\n",
    "5. 다음 층에 있는 유닛으로 출력을 보낸다.\n",
    "- 각 은닉층과 출력층에 대해 층에 사용할 유닛 개수와 활성화 함수를 정의해야 한다. 층에 유닛을 많이 추가할수록 네트워크는 더 복잡한 패턴을 학습할 수 있으나 과대적합이 될 수 있다. 은닉층의 활성화 함수는 주로 렐루이다.      \n",
    "    $f(z)=max(0,z)$, &nbsp; $z$ = 가중치가 적용된 입력과 절편의 합.\n",
    "- 네트워크에 사용할 여러 은닉층을 정의. 층이 많으면 복잡한 관계 학습 가능. 그러나 계산 비용 증가.\n",
    "- 출력층의 활성화 함수 정의. 출력 형태는 네트워크의 목적에 따라 결정. 종류: 이진분류(시그모이드 함수와 하나의 유닛), 다중 분류(소프트맥스 활성화 함수와 k(타깃 클래스 개수)개의 유닛), 회귀(활성화 함수x, 하나의 유닛)\n",
    "- 손실 함수또한 정의해야 한다. 문제의 유형에 따라 결정. 종류: 이진 분류(이진 크로스엔트로피), 다중 분류(범주형 크로스엔트로피), 회귀(평균 제곱 오차)\n",
    "- 옵티마이저를 정의해야 한다. 가장 작은 손실 함수 오차를 만드는 파라미터 값을 찾는 전략. 주로 확률적 경사 하강법, 모멘텀을 사용한 확률적 경사 하강법, RMSProp, Adam.\n",
    "- 하나 이상의 성능 지표를 선택해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# 신경망 모델 제작.\n",
    "network = models.Sequential()\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결층을 추가.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결층을 추가.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 시그모이드 활성화 함수를 사용한 완전 연결층을 추가.\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 신경망의 모델 설정을 완료.\n",
    "network.compile(loss=\"binary_crossentropy\", # 크로스 엔트로피\n",
    "               optimizer=\"rmsprop\",         # 옵티마이저\n",
    "               metrics=[\"accuracy\"])        # 성능 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이진 분류기 훈련하기\n",
    "이진 분류 신경망을 훈련하고 싶다. → 만든 신경망에 fit method로 훈련하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.8153 - val_loss: 0.3395 - val_accuracy: 0.8566\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 0s 977us/step - loss: 0.3237 - accuracy: 0.8664 - val_loss: 0.3272 - val_accuracy: 0.8612\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 0s 993us/step - loss: 0.3124 - accuracy: 0.8683 - val_loss: 0.3441 - val_accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리를 임포트합니다.\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 랜덤 시드를 설정합니다.\n",
    "np.random.seed(0)\n",
    "\n",
    "# 필요한 특성 개수를 지정합니다.\n",
    "number_of_features = 1000\n",
    "\n",
    "# 영화 리뷰 데이터에서 훈련 데이터와 타깃 벡터를 로드합니다.\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "# 영화 리뷰 데이터를 원-핫 인코딩된 특성 행렬로 변환합니다.\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# 신경망 모델을 만듭니다.\n",
    "network = models.Sequential()\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(\n",
    "    number_of_features,)))\n",
    "\n",
    "# 렐루 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# 시그모이드 활성화 함수를 사용한 완전 연결 층을 추가합니다.\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# 신경망의 모델 설정을 완료합니다.\n",
    "network.compile(loss=\"binary_crossentropy\", # 크로스 엔트로피\n",
    "                optimizer=\"rmsprop\", # 옵티마이저\n",
    "                metrics=[\"accuracy\"]) # 성능 지표\n",
    "\n",
    "# 신경망을 훈련합니다.\n",
    "history = network.fit(features_train, # 특성\n",
    "                      target_train, # 타깃 벡터\n",
    "                      epochs=3, # 에포크 횟수\n",
    "                      verbose=1, # 에포크 과정을 출력합니다.\n",
    "                      batch_size=100, # 배치의 샘플 개수\n",
    "                      validation_data=(features_test, target_test)) # 테스트 데이터"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
